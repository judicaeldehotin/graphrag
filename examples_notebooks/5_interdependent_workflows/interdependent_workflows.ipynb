{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f7567f-077c-4fdc-b802-8ac4e34ba6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e394ce-6963-49bd-af5e-782c1fd3bba9",
   "metadata": {},
   "source": [
    "### interdependent_workflows\n",
    "#### init  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f6b86b-efda-4bfd-8aed-3e274421e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "# Our fake dataset\n",
    "dataset = pd.DataFrame([\n",
    "    {\"type\": \"A\", \"col1\": 2, \"col2\": 4},\n",
    "    {\"type\": \"A\", \"col1\": 5, \"col2\": 10},\n",
    "    {\"type\": \"A\", \"col1\": 15, \"col2\": 26},\n",
    "    {\"type\": \"B\", \"col1\": 6, \"col2\": 15},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91408a98-1518-4471-9aee-4f9c80f7ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PIPELINE_YAML = \"\"\"\n",
    "workflows:\n",
    "  - name: aggregate_workflow\n",
    "    steps:\n",
    "      - verb: \"aggregate\"  # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/aggregate.py\n",
    "        args:\n",
    "            groupby: \"type\"\n",
    "            column: \"col_multiplied\"\n",
    "            to: \"aggregated_output\"\n",
    "            operation: \"sum\"\n",
    "        input:\n",
    "          source: \"workflow:derive_workflow\" # reference the derive_workflow, cause this one requires that one to run first\n",
    "            # Notice, these are out of order, the indexing engine will figure out the right order to run them in\n",
    "\n",
    "  - name: derive_workflow\n",
    "    steps:\n",
    "      - verb: \"derive\" # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/derive.py\n",
    "        args:\n",
    "          column1: \"col1\"  # from above\n",
    "          column2: \"col2\"  # from above\n",
    "          to: \"col_multiplied\"  # new column name\n",
    "          operator: \"*\"  # multiply the two columns,\n",
    "    # Since we're trying to act on the dataset, we don't need explicitly to specify an input\n",
    "      # \"input\": { \"source\": \"source\" } # use the dataset as the input to this verb. This is the default, so you can omit it.\n",
    "\n",
    "\"\"\"\n",
    "pipeline_file =  Path().cwd() / \"pipeline.yaml\"\n",
    "with pipeline_file.open(\"w\") as file:\n",
    "    file.write(PIPELINE_YAML)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9e1589-86bc-4124-a3b5-7052aca21ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/5_interdependent_workflows/pipeline.yaml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your config without the input section\n",
    "config_path = str(pipeline_file)\n",
    "config_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86915937-137d-43fa-b5db-cc43e0822924",
   "metadata": {},
   "source": [
    "#### start pipeline workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc36274a-b951-4a82-8f89-2316d8139a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  type  aggregated_output\n",
      "0    A                448\n",
      "1    B                 90\n"
     ]
    }
   ],
   "source": [
    "from graphrag.index import run_pipeline_with_config\n",
    "tables = []\n",
    "async for table in run_pipeline_with_config(\n",
    "    config_or_path=config_path, dataset=dataset\n",
    "):\n",
    "    tables.append(table)\n",
    "pipeline_result = tables[-1]\n",
    "\n",
    "if pipeline_result.result is not None:\n",
    "    # Should look something like this, which should be identical to the python example:\n",
    "    #     type  aggregated_output\n",
    "    # 0    A                448\n",
    "    # 1    B                 90\n",
    "    print(pipeline_result.result)\n",
    "else:\n",
    "    print(\"No results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330be5fe-e0de-4e5a-9683-f63d7eab9757",
   "metadata": {},
   "source": [
    "#### use Python API flow pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a70715-8ac1-4801-9935-e5d73acede6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag.index.config import PipelineWorkflowReference\n",
    "\n",
    "workflows: list[PipelineWorkflowReference] = [\n",
    "    PipelineWorkflowReference(\n",
    "        name=\"aggregate_workflow\",\n",
    "        steps=[\n",
    "            {\n",
    "                \"verb\": \"aggregate\",  # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/aggregate.py\n",
    "                \"args\": {\n",
    "                    \"groupby\": \"type\",\n",
    "                    \"column\": \"col_multiplied\",\n",
    "                    \"to\": \"aggregated_output\",\n",
    "                    \"operation\": \"sum\",\n",
    "                },\n",
    "                \"input\": {\n",
    "                    \"source\": \"workflow:derive_workflow\",  # reference the derive_workflow, cause this one requires that one to run first\n",
    "                    # Notice, these are out of order, the indexing engine will figure out the right order to run them in\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    PipelineWorkflowReference(\n",
    "        name=\"derive_workflow\",\n",
    "        steps=[\n",
    "            {\n",
    "                # built-in verb\n",
    "                \"verb\": \"derive\",  # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/derive.py\n",
    "                \"args\": {\n",
    "                    \"column1\": \"col1\",  # from above\n",
    "                    \"column2\": \"col2\",  # from above\n",
    "                    \"to\": \"col_multiplied\",  # new column name\n",
    "                    \"operator\": \"*\",  # multiply the two columns,\n",
    "                },\n",
    "                # Since we're trying to act on the default input, we don't need explicitly to specify an input\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72083b8b-9a8c-4941-9108-afd506ff2eb9",
   "metadata": {},
   "source": [
    "##### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d327f8-72bc-4f16-8202-b6398301fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  type  aggregated_output\n",
      "0    A                448\n",
      "1    B                 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from graphrag.index import run_pipeline\n",
    "# Grab the last result from the pipeline, should be our aggregate_workflow since it should be the last one to run\n",
    "tables = []\n",
    "async for table in run_pipeline(dataset=dataset, workflows=workflows):\n",
    "    tables.append(table)\n",
    "pipeline_result = tables[-1]\n",
    "\n",
    "if pipeline_result.result is not None:\n",
    "    # Should look something like this:\n",
    "    #     type  aggregated_output\n",
    "    # 0    A                448\n",
    "    # 1    B                 90\n",
    "\n",
    "    # This is because we first in \"derive_workflow\" we multiply col1 and col2 together, then in \"aggregate_workflow\" we sum them up by type\n",
    "    print(pipeline_result.result)\n",
    "else:\n",
    "    print(\"No results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781aaf3-8478-4a64-bb63-479d2de23003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
